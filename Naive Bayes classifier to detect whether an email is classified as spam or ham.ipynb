{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd78ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import makedirs, path, remove, rename, rmdir\n",
    "from tarfile import open as open_tar\n",
    "from shutil import rmtree\n",
    "from urllib import request, parse\n",
    "from glob import glob\n",
    "from os import path\n",
    "from re import sub\n",
    "from email import message_from_file\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066917d",
   "metadata": {},
   "source": [
    "### Downloading the emails to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cadbd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus(dataset_dir: str = 'data'):\n",
    "    base_url = 'https://spamassassin.apache.org'\n",
    "    corpus_path = 'old/publiccorpus'\n",
    "    files = {\n",
    "        '20021010_easy_ham.tar.bz2': 'ham',\n",
    "        '20021010_hard_ham.tar.bz2': 'ham',\n",
    "        '20021010_spam.tar.bz2': 'spam',\n",
    "        '20030228_easy_ham.tar.bz2': 'ham',\n",
    "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
    "        '20030228_hard_ham.tar.bz2': 'ham',\n",
    "        '20030228_spam.tar.bz2': 'spam',\n",
    "        '20030228_spam_2.tar.bz2': 'spam',\n",
    "        '20050311_spam_2.tar.bz2': 'spam' }\n",
    "\n",
    "    #creates the folders: downloads, ham and spam\n",
    "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
    "    ham_dir = path.join(dataset_dir, 'ham')\n",
    "    spam_dir = path.join(dataset_dir, 'spam')\n",
    "\n",
    "    makedirs(downloads_dir, exist_ok=True)\n",
    "    makedirs(ham_dir, exist_ok=True)\n",
    "    makedirs(spam_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for file, spam_or_ham in files.items():\n",
    "        # download files from URL of each specific .bz2 file\n",
    "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
    "        tar_filename = path.join(downloads_dir, file)\n",
    "        request.urlretrieve(url, tar_filename)\n",
    "\n",
    "        #list e-mails in the compressed .bz2 file\n",
    "        emails = []\n",
    "        with open_tar(tar_filename) as tar:\n",
    "            tar.extractall(path=downloads_dir)\n",
    "            for tarinfo in tar:\n",
    "                if len(tarinfo.name.split('/')) > 1:\n",
    "                    emails.append(tarinfo.name)\n",
    "\n",
    "        # move e-mails to ham or spam directory\n",
    "        for email in emails:\n",
    "            directory, filename = email.split('/')\n",
    "            directory = path.join(downloads_dir, directory)\n",
    "\n",
    "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
    "                rename(path.join(directory, filename),\n",
    "                   path.join(dataset_dir, spam_or_ham, filename))\n",
    "\n",
    "        rmtree(directory)\n",
    "\n",
    "download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bb444",
   "metadata": {},
   "source": [
    "### How many e-mails are classified in our dataset as either Spam or not Spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5cbf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Non-Spam E-mails: 6952\n",
      "\n",
      "Number of Spam E-mails: 2399\n"
     ]
    }
   ],
   "source": [
    "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
    "ham_dir = path.join('data', 'ham')\n",
    "spam_dir = path.join('data', 'spam')\n",
    "\n",
    "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))\n",
    "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c6646",
   "metadata": {},
   "source": [
    "### Classifier based on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4634e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9727418492784607\n",
      "Precision: 0.993103448275862\n",
      "Recall: 0.9\n",
      "F1 Score: 0.9442622950819672\n"
     ]
    }
   ],
   "source": [
    "# Function to read emails from files\n",
    "def read_emails(directory):\n",
    "    emails = []\n",
    "    for file in listdir(directory):\n",
    "        with open(path.join(directory, file), 'r', encoding='latin1') as f:\n",
    "            try:\n",
    "                email = message_from_file(f)\n",
    "                body = ''\n",
    "                if email.is_multipart():\n",
    "                    for part in email.walk():\n",
    "                        if part.get_content_type() == 'text/plain':\n",
    "                            body += part.get_payload()\n",
    "                else:\n",
    "                    body = email.get_payload()\n",
    "                emails.append(body)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file}: {e}\")\n",
    "    return emails\n",
    "\n",
    "# Directories\n",
    "dataset_dir = 'data'\n",
    "download_corpus(dataset_dir)\n",
    "\n",
    "# Read emails\n",
    "#ham_dir = path.join(dataset_dir, 'ham')\n",
    "#spam_dir = path.join(dataset_dir, 'spam')\n",
    "\n",
    "ham_dir = path.join('data', 'ham')\n",
    "spam_dir = path.join('data', 'spam')\n",
    "\n",
    "ham_emails = read_emails(ham_dir)\n",
    "spam_emails = read_emails(spam_dir)\n",
    "\n",
    "# Create labels\n",
    "ham_labels = np.zeros(len(ham_emails))\n",
    "spam_labels = np.ones(len(spam_emails))\n",
    "\n",
    "# Concatenate ham and spam emails\n",
    "all_emails = ham_emails + spam_emails\n",
    "all_labels = np.concatenate([ham_labels, spam_labels])\n",
    "\n",
    "# Vectorize emails\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(all_emails)\n",
    "\n",
    "# Train-test split\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(stratified_split.split(X, all_labels))\n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
